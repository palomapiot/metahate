{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "724cbc8e554a776a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import logging\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "import gensim.corpora as corpora\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['rt', 'https'])\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ffe5f52c379479"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "954f511a4f3c88b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/metahate.csv', sep='\\t', names=['label', 'text'])\n",
    "\n",
    "text_hate = data.loc[data['label'] == 1, 'text'].tolist()\n",
    "text_no_hate = data.loc[data['label'] == 0, 'text'].tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "369d6201830fb5e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We create one LDA model for hate data an another for non-hate data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccc5d23b6b4b12a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_data = text_hate # Late `text_no_hate`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62b724df57771e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing and preprocess the text data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e90e1f615fae08d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into words.\n",
    "\n",
    "    Parameters:\n",
    "    - sentences (list): List of sentences to tokenize.\n",
    "\n",
    "    Yields:\n",
    "    - list: A list of words for each sentence.\n",
    "    \"\"\"\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\s+', ' ', str(sent))  # Remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", str(sent))  # Remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield sent\n",
    "\n",
    "data_words = list(sent_to_words(text_data))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed1e99f515f5159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating n-grams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b50ad3ee92e176e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(\n",
    "    data_words, \n",
    "    min_count=5, # Ignores all words and bigrams with total collected count lower than this\n",
    "    threshold=100\n",
    ") \n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f6dc9601c1aad37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing, preprocessing, and lemmatizing each document"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43afcb95e3c993c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Tokenize, preprocess, and lemmatize a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): List of tokenized texts to process.\n",
    "    - stop_words (set): Set of stop words to remove during processing.\n",
    "    - allowed_postags (list): List of allowed POS (Part-of-Speech) tags.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of processed and lemmatized texts.\n",
    "    \"\"\"\n",
    "    # Remove stop words from each document and apply bigram and trigram models\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    # Remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "296c506abccd7707"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the dictionary and corpus for topic modeling using Gensim"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "191153a8088017ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_ready)\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347c1754c2769d15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and training an LDA (Latent Dirichlet Allocation) model using Gensim"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec2cc7fedd82ce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, # Bag-of-words representation of the documents\n",
    "                                           id2word=id2word, # Dictionary mapping words to unique numerical IDs\n",
    "                                           num_topics=8, # Number of topics to identify\n",
    "                                           random_state=100, # Random seed for reproducibility\n",
    "                                           update_every=1, #  often the model parameters should be updated\n",
    "                                           chunksize=100, # Number of documents to be used in each training chunk\n",
    "                                           passes=10, # Number of passes through the entire corpus during training\n",
    "                                           alpha=0.31, # Parameter controlling the document-topic density\n",
    "                                           eta=0.9, # Parameter controlling the topic-word density\n",
    "                                           iterations=100, # Maximum number of iterations through the corpus when inferring topic distributions\n",
    "                                           per_word_topics=True) # Compute a list of topics, each represented by a list of words and associated probabilities\n",
    "joblib.dump(lda_model, 'lda_model.jl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c221428bef369641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating the dominant topics and its percentage contribution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24217052bfac1a8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_row(row_list):\n",
    "    \"\"\"\n",
    "    Process a row of LDA model output.\n",
    "\n",
    "    Parameters:\n",
    "    - row_list (list): List representing the output for a document from the LDA model.\n",
    "\n",
    "    Returns:\n",
    "    - list: Processed information including the dominant topic number, proportion, and keywords.\n",
    "    \"\"\"\n",
    "    row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "    row = sorted(row, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if row:\n",
    "        topic_num, prop_topic = row[0]\n",
    "        wp = lda_model.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        return [int(topic_num), round(prop_topic, 4), topic_keywords]\n",
    "\n",
    "    return [None, None, None]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e92ebbddd0766cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sent_topics_df = pd.DataFrame()\n",
    "\n",
    "# Explicitly iterate through the rows of the LDA model corpus and apply the processing function\n",
    "for i, row_list in tqdm(enumerate(lda_model[corpus])):\n",
    "    sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([process_row(row_list)],\n",
    "                                                             columns=['Dominant_Topic', 'Topic_Perc_Contrib',\n",
    "                                                                      'Keywords'])], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e27be41adf9754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "contents = pd.Series(data_ready)\n",
    "sent_topics_df['Text'] = contents\n",
    "sent_topics_df = sent_topics_df.dropna(subset=['Dominant_Topic'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b74202853b46b3e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f1e247033124b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution of document word counts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "795f01f9b1817a2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in sent_topics_df.Text]\n",
    "\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, bins = 1000, color='#3d3d3d')\n",
    "plt.text(800, 32000, 'Mean: ' + str(round(np.mean(doc_lens))))\n",
    "plt.text(800,  28000, 'Median: ' + str(round(np.median(doc_lens))))\n",
    "plt.text(800,  23000, 'Standard deviation: ' + str(round(np.std(doc_lens))))\n",
    "plt.text(800,  18000, '1% quantile: ' + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(800,  13000, '99% quantile: ' + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1700acb96b32208c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution of document word counts by topic"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa1a6464f6ec22c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colour = '#000'\n",
    "\n",
    "fig, axes = plt.subplots(2,3,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    df_dominant_topic_sub = sent_topics_df.loc[sent_topics_df.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, bins=1000, color=colour)\n",
    "    ax.tick_params(axis='y', labelcolor=colour, color=colour)\n",
    "    sns.kdeplot(doc_lens, color='black', shade=False, ax=ax.twinx())\n",
    "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=colour)\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=colour))\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ae7326221ac8f71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wordcloud"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ff5edd6bbe965f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colour = '#000'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: colour,\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=450)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=18))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a86e2558bfbde3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word count and importance of topic keywords"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b1331388840b9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16,10), sharey=True, dpi=160)\n",
    "colour = '#000'\n",
    "\n",
    "topics = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==topics[i], :], color=colour, width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==topics[i], :], color=colour, width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=colour)\n",
    "    ax_twin.set_ylim(0, 0.3); ax.set_ylim(0, 135000)\n",
    "    ax.set_title('Topic: ' + str(i), color=colour, fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==topics[i], 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959f1e691d43c3ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### t-SNE clustering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114e188c3fd8b77b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "arr = arr[np.amax(arr, axis=1) > 0.5]\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca', perplexity=40)\n",
    "tsne_lda = tsne_model.fit_transform(arr)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4a13a767fe5ee34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6107b6a85b34c56b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colors = np.array([\"#c7e9c0\", \"#a1d99b\", \"#74c476\", \"#31a354\", \"#006d2c\", \"#003816\"])\n",
    "plt.figure(figsize=(8, 8), dpi=300)\n",
    "scatter = plt.scatter(tsne_lda[:,0], tsne_lda[:,1], c=colors[topic_num], s=8)\n",
    "\n",
    "handles, _ = scatter.legend_elements(prop='colors')\n",
    "plt.suptitle(\"t-SNE Clustering\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d3717ee0642d92d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
